{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTTMXcJSKI7g",
        "outputId": "3bf5d07d-630a-4b47-f10f-b55a4176d0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.9 (from langchain)\n",
            "  Downloading langchain_community-0.0.11-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
            "  Downloading langchain_core-0.1.10-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Downloading langsmith-0.0.80-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.11 langchain-core-0.1.10 langsmith-0.0.80 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NruVfOq4Lgqa",
        "outputId": "c3559c71-ed45-45df-ebcd-5d8c1950b595"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.7.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.7.1 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfRGDXNVLi69",
        "outputId": "a58210fa-0ac3-46cf-93a1-40f5f39d57bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_KEY='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'\n"
      ],
      "metadata": {
        "id": "LArU6RGAM8nl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai, langchain\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv\n",
        "api_key=os.getenv(\"OPENAI_KEY\",None)"
      ],
      "metadata": {
        "id": "jgxvaX3oNEoY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an application (Basic)"
      ],
      "metadata": {
        "id": "1MorExABL3DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm=OpenAI(temperature=0.4, openai_api_key=OPENAI_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIcOGn72Lre2",
        "outputId": "4a4abf30-da58-4c0e-938b-3a419c75eef4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step would be to predict by passing in the text as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "-XLU2lyPOip5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm.predict(\"Suggest me the skill that it in demand?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O__qV1SrMVnC",
        "outputId": "3536a368-b83b-46d9-8ac1-dfd0bf1223c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One skill that is currently in high demand is data analysis. With the increasing use of technology and the collection of large amounts of data, companies are looking for individuals who can effectively analyze and interpret this data to make informed business decisions. This skill involves being able to collect, organize, and analyze data using tools such as Excel, SQL, and Tableau. It also requires critical thinking and problem-solving abilities to identify patterns and insights from the data. Data analysis is a highly sought-after skill in various industries, including finance, marketing, healthcare, and technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# That is it! The response generated is as follows:\n",
        "One skill that is currently in high demand is data analysis. With the rise of big data and the increasing importance of data-driven decision making in businesses, there is a growing need for individuals who can collect, organize, and analyze large amounts of data to extract valuable insights. This skill is sought after in a variety of industries, including finance, marketing, healthcare, and technology. Additionally, proficiency in data analysis tools and programming languages such as Python, R, and SQL is highly desirable.\n",
        "**bold text**\n"
      ],
      "metadata": {
        "id": "vplXnq9fPCA3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5EKbsBvpPBgn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt template**\n",
        "\n",
        "A prompt template is a way of creating prompts for language models using placeholders and input values. A prompt is the input that guides the model’s response. A prompt template can have different sections, such as instructions, context, examples, and output indicator. A prompt template can be formatted with different input values to create different prompts for different tasks or scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uf-ASdXfYmYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, suppose you want to create a prompt template for generating jokes about different topics. You can use the following template:\n",
        "\n"
      ],
      "metadata": {
        "id": "2Mo9QC32Y3f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tell me a {adjective} joke about {content}.\n"
      ],
      "metadata": {
        "id": "BZ3yHd-lZBk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, {adjective} and {content} are placeholders that can be replaced with different values. For example, you can format the template with the values adjective=\"funny\" and content=\"math\" to create the following prompt:"
      ],
      "metadata": {
        "id": "9Shw_22hZG9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then pass this prompt to a language model and get a response, such as:\n",
        "\n"
      ],
      "metadata": {
        "id": "1NXr85_OZLyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do mathematicians always confuse Halloween and Christmas?\n",
        "Because Oct 31 = Dec 25.\n"
      ],
      "metadata": {
        "id": "q5mbZEPVZN0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use different templating engines, such as Python’s str.format or Jinja2, to create and format prompt templates. LangChain provides a class called PromptTemplate that allows you to create and work with prompt templates easily. You can also use conditional logic, loops, filters, and other features to make your prompt templates more dynamic and flexible."
      ],
      "metadata": {
        "id": "vVydTsx4ZRA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import  PromptTemplate, OpenAI\n",
        "\n",
        "# Define a simple prompt template as a Python string\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "Human: What is the capital of {place}?\n",
        "AI: The capital of {place} is {capital}\n",
        "\"\"\")\n",
        "prompt = prompt_template.format(place=\"California\", capital=\"Sacramento\")\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Tm-kkUYp2h",
        "outputId": "0a14bcb4-4637-45bf-e6c2-321014849c3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Human: What is the capital of California?\n",
            "AI: The capital of California is Sacramento\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This above text is prompt which we are going to feed to model(LLM)"
      ],
      "metadata": {
        "id": "cVoTCwQSjKQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_KEY='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'\n"
      ],
      "metadata": {
        "id": "bbARlmuGkqky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate.from_template(\n",
        "    template=\"Write a {length} story about:{content}\"\n",
        ")\n",
        "\n",
        "#llm = OpenAI(OpenAI(temperature=0.4, openai_api_key=OPENAI_KEY))\n",
        "llm=OpenAI(temperature=0.4, openai_api_key=OPENAI_KEY)\n",
        "\n",
        "prompt= prompt_template.format(\n",
        "    length=\"2-sentence\",\n",
        "    content=\"The hometown of the legendrary data scientist,Harpreet Sahota\"\n",
        ")\n",
        "\n",
        "response = llm.predict(\n",
        "    text=prompt\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR_V7Az8Y56N",
        "outputId": "33526bfe-9312-46c3-bfd6-39ccad49dee5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Harpreet Sahota, the renowned data scientist, grew up in a small town nestled in the mountains, where he first discovered his passion for numbers and patterns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate.from_template(\n",
        "    template=\"Write a {length} story about:{content}\"\n",
        ")\n",
        "\n",
        "#llm = OpenAI(OpenAI(temperature=0.4, openai_api_key=OPENAI_KEY))\n",
        "llm=OpenAI(temperature=0.4, openai_api_key=OPENAI_KEY)\n",
        "\n",
        "prompt= prompt_template.format(\n",
        "    length=\"12-sentence\",\n",
        "    content=\"The hometown of the legendrary data scientist,Harpreet Sahota\"\n",
        ")\n",
        "\n",
        "response = llm.predict(\n",
        "    text=prompt\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXtgLr9SkN2U",
        "outputId": "d5b05db4-690b-409b-a511-e4b52992966e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Harpreet Sahota was a renowned data scientist, known for her groundbreaking work in the field of artificial intelligence. She had achieved global recognition for her contributions to the field and had become a legend in the world of data science. But few knew about her humble beginnings in her hometown, a small village nestled in the foothills of the Himalayas.\n",
            "\n",
            "Harpreet grew up in a close-knit community where everyone knew each other. Her parents were farmers, and she spent her childhood playing in the fields and exploring the nearby forests. But even at a young age, Harpreet showed a keen interest in numbers and patterns, often spending hours solving math problems.\n",
            "\n",
            "As she grew older, Harpreet's passion for data and technology only intensified. She would spend hours tinkering with computers and learning coding languages. Her determination and hard work paid off when she received a scholarship to study at a prestigious university in the city.\n",
            "\n",
            "Despite her success and fame, Harpreet never forgot her roots. She often returned to her hometown, where she would share her knowledge and inspire the younger generation to pursue their dreams. The villagers were proud to call her one of their own and would often refer to her as the \"pride of the village.\"\n",
            "\n",
            "Years went by, and Harpre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p4VoEkbNmDtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can instantiate a prompt template with no input variables, one input variable, or multiple input variables, like so:**"
      ],
      "metadata": {
        "id": "rkMBy7S2lu8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No Input Variable\n",
        "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\n",
        "print(no_input_prompt.format())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXJ5hM3ylir3",
        "outputId": "b45b6233-7480-4cf9-febb-a32e2afff154"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a joke.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One Input Variable\n",
        "one_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\n",
        "print(one_input_prompt.format(adjective=\"funny\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQslIUnHlyke",
        "outputId": "3f08bd68-4242-4c90-a1aa-c05b857968ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a funny joke.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple Input Variables\n",
        "multiple_input_prompt = PromptTemplate(\n",
        " input_variables=[\"adjective\", \"content\"],\n",
        " template=\"Tell me a {adjective} joke about {content}.\"\n",
        ")"
      ],
      "metadata": {
        "id": "_gEKuKQil1BD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_input_prompt = multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")\n",
        "print(multiple_input_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHoa54ycl3XF",
        "outputId": "988182e3-5461-4ea1-c61d-c9f720a63b90"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a funny joke about chickens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which will output the following:**\n",
        "* Tell me a joke.\n",
        "* Tell me a funny joke.\n",
        "* Tell me a funny joke about chickens."
      ],
      "metadata": {
        "id": "vjNBIuxFmR2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And pass this to an LLM like so:\n",
        "\n"
      ],
      "metadata": {
        "id": "WJFgfCbHmO7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.predict(\n",
        "    text=multiple_input_prompt\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsyZq3UMl7fo",
        "outputId": "a549ee3a-6107-44a4-ab8e-7bae9c1c22cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Why did the chicken go to the seance?\n",
            "\n",
            "To talk to the other side of the road!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.predict(\n",
        "    text=multiple_input_prompt\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aokFQ1f8mpAj",
        "outputId": "e2546cf9-f54f-433e-cc58-5422b91901cc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Why did the chicken cross the playground?\n",
            "\n",
            "To get to the other slide!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why would I even use a prompt template?**\n",
        "Here are some practical use cases for using a prompt template rather than passing a plain prompt to a language model:\n",
        "\n",
        "## **Reusability**\n",
        "Prompt templates allow you to define a template once and reuse it in multiple places. This avoids duplicating the same prompt logic over and over. For example, you could create a “summarize article” template and reuse it anytime you want a summary.\n",
        "\n",
        "## **Separation of concerns**\n",
        "Prompt templates separate the prompt formatting from the model invocation. This makes the code more modular — you can change the template or the model independently.\n",
        "\n",
        "## **Dynamic prompts**\n",
        "Templates allow you to dynamically generate prompts by filling in template variables. This is useful when you want to customize the prompt based on user input or other runtime factors.\n",
        "\n",
        "## **Readability**\n",
        "Templates can improve readability by encapsulating complex prompt logic in a simple interface. Named variables are often clearer than trying to embed logic directly in strings.\n",
        "\n",
        "## **Maintenance**\n",
        "Changes to shared prompt logic only need to happen in one place rather than everywhere a prompt is defined. This improves maintainability.\n",
        "\n",
        "So in summary, prompt templates improve reusability, modularity and maintenance of prompt engineering code compared to using raw prompt strings directly."
      ],
      "metadata": {
        "id": "aqG6kbYRnLsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"What is the capital of {country}?\"),\n",
        "    (\"ai\", \"The capital of {country} is {capital}.\")\n",
        "])\n",
        "\n",
        "messages = chat_template.format_messages(\n",
        "    country=\"Canada\",\n",
        "    capital=\"Winnipeg\"\n",
        ")\n",
        "\n",
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlUWT56BmyGZ",
        "outputId": "39738cdd-a2da-4e86-945a-1c4aed962ebc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='What is the capital of Canada?'), AIMessage(content='The capital of Canada is Winnipeg.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "Throughout our exploration of **PromptTemplates** in LangChain, one thing becomes undeniably clear: the true power of a language model isn’t just in its underlying architecture but in how we communicate with it.\n",
        "\n",
        "**PromptTemplates** are not merely tools; they are the refined language through which we converse with sophisticated AI systems, ensuring precision, clarity, and adaptability in every interaction.\n",
        "\n",
        "LangChain’s introduction of such a structured approach to prompts marks a significant step forward in the AI domain.\n",
        "\n",
        "By emphasizing reusability, dynamism, and modularity, LangChain ensures that developers can maximize the efficacy of their language model interactions without getting bogged down by complexities.\n",
        "\n",
        "As we move forward in this AI-driven era, tools like **PromptTemplates** will undoubtedly play a pivotal role in defining the boundaries of what’s possible. They stand as a testament to the fact that, while the evolution of AI is essential, the methods we employ to interact with it are equally crucial.\n",
        "\n",
        "With LangChain and **PromptTemplates** at our disposal, the future of seamless, impactful, and meaningful AI interactions looks incredibly bright."
      ],
      "metadata": {
        "id": "r5qom7b9n_9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9BcR-cgooPHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate.from_template(\n",
        "    template=\"Write a {length} story about:{content}\"\n",
        ")\n",
        "\n",
        "#llm = OpenAI(OpenAI(temperature=0.4, openai_api_key=OPENAI_KEY))\n",
        "llm=OpenAI(temperature=0.4, openai_api_key=OPENAI_KEY)\n",
        "\n",
        "prompt= prompt_template.format(\n",
        "    length=\"12-sentence\",\n",
        "    content=\"The hometown of the legendrary machine learning resercher and engineer,jay alammar\"\n",
        ")\n",
        "\n",
        "response = llm.predict(\n",
        "    text=prompt\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yITTFKlonzUC",
        "outputId": "44df2f08-8d74-4b7d-d944-84fe82b21497"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Jay Alammar was born and raised in the small town of Oakwood, nestled in the rolling hills of the Midwest. From a young age, he showed a natural aptitude for mathematics and science, spending hours tinkering with gadgets and machines in his backyard workshop. As he grew older, his passion for technology only intensified, and he quickly became known as the town's resident genius. Despite his small-town roots, Jay's talents caught the attention of top universities, and he eventually landed a coveted spot at MIT. But no matter how far he traveled, Jay never forgot his humble beginnings in Oakwood. In fact, it was during a visit back home that he stumbled upon a groundbreaking idea that would change the field of machine learning forever. With his hometown as his inspiration, Jay poured his heart and soul into his research, eventually becoming one of the most renowned experts in the field. And though he now travels the world giving lectures and consulting for major tech companies, Jay always makes sure to return to Oakwood, where his legend began. The town proudly displays his achievements on a billboard at the entrance, and the local school even named their science lab after him. To this day, Jay credits his success to the support and encouragement he received from his hometown, and he remains\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-w1Nb_0paHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}